<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hybrid Frequency Modeling for Time Series Forecasting</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="css/style.css" />
  </head>
  <body>
    <div class="container">
      <!-- Header -->
      <header>
        <h1>Hybrid Frequency Modeling for Time Series Forecasting</h1>
        <p class="subtitle">
          This project implements and evaluates FreqHybrid, a frequency-aware
          hybrid architecture for time series forecasting. FreqHybrid combines
          LSTM, Transformer, and linear components to handle different frequency
          components of time series data.
        </p>
        <div class="author-info">
          <p>
            <strong>Tina Zhang</strong> | December 2025 | 6.7960 (Deep Learning)
          </p>
        </div>
      </header>

      <!-- Abstract -->
      <section class="abstract">
        <h2>Abstract</h2>
        <p>
          We propose FreqHybrid, a frequency-aware hybrid architecture that
          combines LSTM, Transformer, and linear components for electricity load
          and weather forecasting. Despite its design with explicit frequency
          decomposition, architectural specialization, and adaptive gating,
          FreqHybrid consistently underperforms the simple DLinear baseline by
          15-65% across multiple datasets and horizons. We attribute
          FreqHybrid's poor performance to the high linearity of the datasets,
          the difficulty of optimizing multiple branches, excessive model
          capacity relative to the task, and weak seasonal patterns for the
          Transformer branch to help. These findings clarify when added
          architectural complexity is useful and highlight the importance of
          strong linear baselines in time series forecasting.
        </p>
      </section>

      <!-- Table of Contents -->
      <nav class="toc">
        <h3>Contents</h3>
        <ol>
          <li><a href="#introduction">Introduction</a></li>
          <li><a href="#background">Background and Related Work</a></li>
          <li><a href="#method">Methodology</a></li>
          <li><a href="#implementation">Implementation</a></li>
          <li><a href="#results">Results</a></li>
          <li><a href="#analysis">Analysis</a></li>
          <li><a href="#discussion">Discussion and Future Work</a></li>
          <li><a href="#references">Works Cited</a></li>
        </ol>
      </nav>

      <!-- Section 1: Introduction -->
      <section id="introduction">
        <h2><span class="section-number">1.</span> Introduction</h2>

        <p>
          Time series forecasting is fundamental for understanding and managing real-world systems. Electricity and weather data are challenging because they contain structure at multiple time scales: slow changes over seasons, strong daily or weekly rhythms, and short-term fluctuations from noise and complex interactions.
        </p>

        <p>
          Recent work has shown that increasing model complexity does not always
          improve forecasting accuracy. Zeng et al. (2023) demonstrated that
          DLinear, a simple linear model that separately learns trend and seasonal components, outperforms many advanced Transformer architectures on long-horizon forecasting tasks. This finding raises an important question: instead of making models deeper or more complicated, is it possible to combine different modeling ideas in a way that is more targeted and effective?
        </p>

        <p>
          We explore the idea that forecasting may improve when each part of a time series is handled by a model that naturally fits its behavior. Slow trends may be better captured by LSTMs, repeating patterns by Transformers, and simple linear relationships by linear layers. We introduce FreqHybrid, a frequency-aware hybrid model that decomposes each input series using a moving average, then routes the trend to an LSTM, seasonality to a Transformer, and the full sequence to a linear branch. A learnable gating mechanism adaptively combines the three outputs.
        </p>

        <p>
          We evaluate FreqHybrid on standard electricity and weather forecasting benchmarks across multiple horizons and analyze when hybrid architectures are likely to help or hurt performance.
        </p>
      </section>

      <!-- Section 2: Background -->
      <section id="background">
        <h2>
          <span class="section-number">2.</span> Background and Related Work
        </h2>

        <h3>2.1 Traditional Time Series Forecasting</h3>
        <p>
          Classical forecasting methods such as ARIMA (Adhikari, 2013) and exponential smoothing (Abrami et al., 2017) have historically served as strong baselines. They work well when data is roughly linear, stationary, or dominated by clear seasonal patterns, but struggle with nonlinear dynamics, high-dimensional inputs, and long-range dependencies.
        </p>

        <h3>2.2 Neural Approaches to Time Series</h3>
        <p>
          Deep learning introduced models capable of learning nonlinear relationships directly from data. LSTMs became popular because their gating mechanisms allow them to capture both short- and long-term temporal dependencies (Elsworth & Guttel, 2020), though their improvements over statistical models are often modest. Transformers replaced recurrence with self-attention, allowing direct modeling of long-range interactions. Several Transformer variants such as Informer (Zhou et al., 2020), Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2021), and PatchTST (Nie et al., 2022) adapted attention mechanisms to time series forecasting, but their gains are inconsistent, especially on long-horizon forecasting tasks where simple baselines sometimes perform just as well or better.
        </p>

        <h3>2.3 The DLinear Challenge</h3>
        <p>
          A major shift occurred with DLinear (Zeng et al., 2023), a surprisingly simple model that decomposes a series into trend and seasonal components and applies separate linear layers to each. Despite its simplicity, DLinear outperformed many complex Transformer-based models on long-term forecasting. Follow-up studies have shown that linear models remain highly competitive on datasets with smooth trends and clear periodic patterns (Zeng et al., 2023; Ekambaram et al., 2023; Das et al., 2023), highlighting the importance of inductive bias over model size or architectural complexity.
        </p>

        <h3>2.4 Hybrid Architectures</h3>
        <p>
          To combine the strengths of different modeling approaches, researchers have explored hybrid designs that mix multiple neural components. Prior work includes combinations of LSTMs with feed-forward networks (Liu et al., 2020), CNN encoders (Qiu et al., 2017), and mixed LSTM–Transformer models (Li et al., 2019; Zhou et al., 2020). These hybrids can provide gains in certain settings, but they often feed the same inputs to each component and lack a clear mechanism to decide which branch should handle which part of the signal.
        </p>

        <h3>2.5 Frequency Decomposition in Time Series</h3>
        <p>
          Frequency-based methods have long been used to analyze multi-scale patterns. Fourier transforms separate a signal into periodic components but do not capture when events occur in time (Oppenheim & Schafer, 2009). Wavelet transforms improve time localization but require careful selection of wavelet families and scales (Daubechies, 1992). Empirical Mode Decomposition provides adaptive, data-driven decompositions but is computationally expensive for large datasets (Huang et al., 1998). A simpler alternative is moving-average decomposition, used in DLinear (Zeng et al., 2023), which is fast, easy to interpret, and performs competitively with more complex decomposition techniques.
        </p>

        <h3>2.6 Our Work in Context</h3>
        <p>
          Our work builds on DLinear's observation that explicit decomposition can simplify the forecasting problem. Instead of applying linear layers to decomposed components, we assign each part of the signal to a model whose inductive bias matches its structure: trend with LSTM, seasonal component with Transformer, and full input with a linear branch. We introduce a learnable gating mechanism that determines how much each branch should contribute, allowing us to evaluate whether frequency-specific specialization provides an advantage over purely linear baselines.
        </p>
      </section>

      <!-- Section 3: Method -->
      <section id="method">
        <h2><span class="section-number">3.</span> Methodology</h2>

        <p>
          FreqHybrid is built on a straightforward idea: different parts of a time series occupy different frequency ranges and benefit from different modeling assumptions. Rather than relying on a single architecture to learn all patterns, we first decompose the input sequence and then route each component to a branch designed to model its structure. The full model contains three parallel branches: an LSTM for trends, a Transformer for seasonal patterns, and a linear layer for the full signal. Their outputs are combined through a gating network that learns how much each branch should contribute.
        </p>

        <h3>3.1 Architecture Overview</h3>
        <p>FreqHybrid follows three design principles: (1) frequency decomposition to separate the signal into trend and seasonal components, (2) architectural specialization so each component is modeled by a branch suited to its behavior, and (3) adaptive fusion where the gating network learns the contribution of each branch. Given an input sequence, the model performs four steps: decompose the signal, process each component with a dedicated branch, compute branch weights through a gating network, and combine the outputs to produce a forecast.</p>

        <div class="figure">
          <img src="images/Time Series Decomposition.png" alt="FreqHybrid Architecture Diagram" />
          <p class="caption">
            <strong>Figure 1:</strong> FreqHybrid architecture. The input time series is decomposed into trend and seasonal components using a moving average. The trend component is processed by an LSTM branch, while the seasonal component is handled by a Transformer branch. A linear branch processes both components. An adaptive gating network combines the three branch outputs using learned weights.
          </p>
        </div>

        <h3>3.2 Series Decomposition</h3>
        <p>
          We use a simple moving average, following DLinear, to separate each input sequence into trend and seasonal components. This method is efficient, differentiable, and requires minimal tuning, providing a clear division between low-frequency structure and higher-frequency residuals.
        </p>

        <h3>3.3 Specialized Branches and Gating</h3>
        <p>
          <strong>LSTM branch for the trend:</strong> The trend component changes gradually over time, and an LSTM captures long-term temporal dependencies, processing the trend sequence and projecting it to the prediction horizon. <strong>Transformer branch for seasonality:</strong> Seasonal patterns involve long-range, periodic interactions. A Transformer encoder uses self-attention to model relationships across all time steps, then pools and maps the representation to the forecast horizon. <strong>Linear branch for the full signal:</strong> Following DLinear, we include a simple linear branch using direct linear projections, providing a strong baseline and stabilizing training.
        </p>
        <p>
          Rather than averaging branch outputs, we use a small MLP as a "gate" that learns how much each branch should contribute. The gate receives the LSTM's final hidden state, the pooled Transformer output, and summary statistics of the trend and seasonal components. It produces three non-negative weights, normalized through softmax, allowing adaptive weighting based on the input. The final prediction is a weighted sum of the three branch outputs, forming a convex combination that ensures stability and interpretability.
        </p>

        <h3>3.4 Training and Model Complexity</h3>
        <p>
          FreqHybrid is trained end to end using mean squared error as the objective, with Adam optimizer, learning rate scheduling, early stopping, and gradient clipping. All components are optimized jointly, allowing each branch to specialize in the patterns it is best suited to represent. FreqHybrid contains roughly 300K parameters, which is larger than DLinear but smaller than common Transformer models. However, because the architecture includes both an LSTM and a Transformer encoder, training and inference are significantly slower than DLinear.
        </p>

      <!-- Section 4: Implementation -->
      <section id="implementation">
        <h2><span class="section-number">4.</span> Implementation</h2>

        <p>
          We evaluate FreqHybrid on two widely used multivariate time series
          datasets, which are time series with multiple variables measured over time, to test whether frequency-aware specialization and adaptive
          gating provide any advantage over strong linear baselines such as
          DLinear.
        </p>

        <h3>Datasets</h3>
        <p>We use two benchmarks: (1) Electricity Load Diagrams (2011-2014), hourly electricity consumption from 321 clients with clear daily and weekly cycles, and (2) Weather, hourly measurements of 21 meteorological variables including temperature, humidity, pressure, and wind. Both datasets are split chronologically into 60% training, 20% validation, and 20% testing.</p>

        <h3>Forecasting Setup</h3>
        <p>All models use a sliding window approach with input length of 96 time steps and forecast horizons of 96, 336, and 720 steps. Electricity experiments use the 96-hour horizon; Weather experiments use all horizons.</p>

        <h3>Model Configuration</h3>
        <div class="config-table">
          <p><strong>FreqHybrid Configuration:</strong> 25-step moving average decomposition. Trend branch: 3-layer LSTM (256 hidden units). Seasonal branch: 3-layer Transformer encoder (d_model=512, 8 attention heads, FFN dim=2048). Linear branch: separate projections. Gating network: 3-layer MLP with softmax-normalized weights. Training: Adam optimizer, LR=3×10⁻⁴ with cosine annealing, batch size=16 (8 for long horizons), up to 15 epochs with early stopping (patience=5), MSE loss.</p>

          <p><strong>DLinear Baseline:</strong> Moving-average decomposition, separate linear layers for trend and seasonality, batch size=16, LR=1×10⁻⁴, early stopping (patience=5). This standardized setup ensures that any performance differences reflect model design rather than training procedure.</p>
        </div>
      </section>

      <!-- Section 5: Results -->
      <section id="results">
        <h2><span class="section-number">5.</span> Results</h2>

        <h3>5.1 Evaluation Metrics</h3>
        <p>
          We evaluate model performance using Mean Squared Error (MSE) and Mean Absolute Error (MAE). MSE gives more weight to larger errors; MAE provides a more interpretable measure of average prediction error.
        </p>

        <div class="metric-formulas">
          <img src="images/mse.png" alt="Mean Squared Error Formula" class="metric-formula" />
          <img src="images/mae.png" alt="Mean Absolute Error Formula" class="metric-formula" />
        </div>

        <h3>5.2 Performance Comparison</h3>
        <div class="alert alert-negative">
          <p>
            <strong> Key Finding:</strong> FreqHybrid consistently underperforms
            DLinear across all tested scenarios, despite its significantly
            greater architectural complexity. Table 1 summarizes the quantitative results across all experiments.
          </p>
        </div>

        <table class="results-table">
          <caption>
            <strong>Table 1:</strong>
            Forecasting Performance Comparison
          </caption>
          <thead>
            <tr>
              <th>Dataset</th>
              <th>Horizon</th>
              <th>DLinear MSE</th>
              <th>DLinear MAE</th>
              <th>FreqHybrid MSE</th>
              <th>FreqHybrid MAE</th>
              <th>Difference</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Electricity</td>
              <td>96h</td>
              <td>0.2065</td>
              <td>0.2948</td>
              <td>0.2881</td>
              <td>0.3694</td>
              <td class="negative">+39.5%</td>
            </tr>
            <tr>
              <td>Weather</td>
              <td>96h</td>
              <td>0.1987</td>
              <td>0.2580</td>
              <td>0.3272</td>
              <td>0.3654</td>
              <td class="negative">+64.6%</td>
            </tr>
            <tr>
              <td>Weather</td>
              <td>336h</td>
              <td>0.2838</td>
              <td>0.3332</td>
              <td>0.3812</td>
              <td>0.4043</td>
              <td class="negative">+34.3%</td>
            </tr>
            <tr>
              <td>Weather</td>
              <td>720h</td>
              <td>0.3461</td>
              <td>0.3818</td>
              <td>0.3987</td>
              <td>0.4144</td>
              <td class="negative">+15.2%</td>
            </tr>
          </tbody>
        </table>

        <div class="figure">
          <img src="images/electricity_bar.png" alt="Performance Comparison" />
          <p class="caption">
            <strong>Figure 2:</strong> Mean squared error (MSE) comparison across datasets and
            forecast horizons. Lower values indicate better performance. DLinear (blue) consistently achieves
            lower error than FreqHybrid (purple).
          </p>
        </div>

        <p>
          On the Electricity dataset at 96-hour horizon, DLinear achieves an MSE of 0.207 and MAE of 0.295, while FreqHybrid obtains MSE of 0.288 and MAE of 0.369 (39.5% increase in MSE). This gap is notable because FreqHybrid uses roughly three times more parameters and requires about six times longer training time per epoch.
        </p>

        <h3>5.3 Performance Difference Analysis</h3>
        <p>
          FreqHybrid consistently underperforms DLinear across all four experimental conditions: Electricity 96h (+39.5% MSE), Weather 96h (+64.6% MSE), Weather 336h (+34.3% MSE), and Weather 720h (+15.2% MSE). While formal statistical significance testing would require individual prediction errors or multiple independent runs, the magnitude and consistency of these differences (15-65% relative increases in MSE) provide strong evidence that the performance gap reflects genuine architectural differences.
        </p>

        <div class="figure">
          <img src="images/train_val_electricity.png" alt="Electricity Training and Validation Curves" />
          <p class="caption">
            <strong>Figure 3:</strong> Training and validation loss curves for the
            Electricity dataset. DLinear converges quickly and smoothly, while FreqHybrid shows slower improvement and higher variability during training.
          </p>
        </div>
        <p>DLinear reaches low training and validation loss within the first few epochs and continues to improve steadily. FreqHybrid begins with significantly higher loss, improves more slowly, and shows noticeable fluctuations, indicating optimization difficulties.</p>

        <div class="figure">
          <img src="images/line_plot.png" alt="Horizon Effect" />
          <p class="caption">
            <strong>Figure 4: </strong> Forecasting mean squared error (MSE) on the Weather dataset across different horizons. The performance gap between DLinear and FreqHybrid decreases at longer horizons, but FreqHybrid remains consistently worse.
          </p>
        </div>
        <p>
As the forecast horizon increases from 96 to 720 hours, both models show higher error. Although the gap between the two models becomes smaller at longer horizons, FreqHybrid never matches DLinear, suggesting that the added complexity does not translate into improved long-range performance.
        </p>

        <div class="figure">
          <img src="images/weather_bar.png" alt="Training Curves" />
          <p class="caption">
            <strong>Figure 5:</strong> MSE on the Weather dataset across three forecasting horizons. DLinear maintains lower error at every horizon, while FreqHybrid consistently underperforms.
          </p>
        </div>

        <p>
          DLinear achieves lower MSE than FreqHybrid at all three horizons, with the difference largest at shorter horizons where local linearity dominates.
        </p>


        <div class="figure">
          <img src="images/heatmap.png" alt="Improvement Heatmap" />
          <p class="caption">
            <strong>Figure 6:</strong> Heatmap showing FreqHybrid's relative performance compared to DLinear. Values indicate percentage difference in MSE (positive means FreqHybrid is better). All cells are negative, which means that DLinear outperforms FreqHybrid at every forecast horizon.
          </p>
        </div>

        <p>
          FreqHybrid performs worse than DLinear across all horizons. The gap is largest at short horizons (-64.6% at 96 hours) and decreases at longer horizons (-34.3% at 336 hours, -15.2% at 720 hours), but FreqHybrid never surpasses the linear baseline.
        </p>

        <div class="figure">
          <img src="images/train_val_weather.png" alt="Training Curves" />
          <p class="caption">
            <strong>Figure 7:</strong> Training and validation loss curves on the Weather dataset. DLinear shows stable, monotonic improvement, while FreqHybrid displays slower convergence and greater variability.
          </p>
        </div>

        <p>
          On the Weather dataset, both models reduce training loss quickly during the first few epochs, but their behaviors diverge afterwards. DLinear continues to decrease both training and validation loss smoothly, while FreqHybrid becomes unstable with oscillations in validation loss and a noticeably higher final error.
        </p>
      </section>

      <!-- Section 6: Analysis -->
      <section id="analysis">
        <h2>
          <span class="section-number">6.</span> Analysis and Discussion
        </h2>

        <p>
          The consistent underperformance of FreqHybrid contradicts our initial expectations that frequency-aware decomposition and architectural specialization would have advantages on datasets with strong periodic patterns. Our experiments suggest that four main factors explain why the hybrid model fails to outperform the simpler DLinear baseline.
        </p>

        <h3>6.1 Dataset Characteristics: Strong Local Linearity</h3>
        <p>
          Both the Electricity and Weather datasets exhibit <strong>strong local linearity</strong> across the forecast windows we examine. Consecutive time steps are very similar, making simple linear extrapolation highly effective. Linear models achieve strong accuracy by learning stable weighted combinations of recent values, while FreqHybrid's nonlinear capabilities add overhead where simple interpolation suffices.
        </p>

        <h3>6.2 Optimization Complexity: Conflicting Gradients</h3>
        <p>
          FreqHybrid's three-branch architecture introduces <strong>significant optimization challenges</strong>. During training, the model must simultaneously learn (1) how to decompose signals, (2) how each branch should represent its component, and (3) how the gate should weight the branches. These learning objectives can produce conflicting gradients. If the gate gives little weight to the seasonal branch, the Transformer receives almost no gradient signal. If the gate starts with balanced weights, the branches compete for credit, slowing convergence. The training curves reflect these difficulties: DLinear's validation loss decreases smoothly and stabilizes by around epoch 8, while FreqHybrid's validation loss shows plateaus, fluctuations, and occasional increases.
        </p>

        <h3>6.3 Model Capacity and Overfitting</h3>
        <p>
          With approximately 300,000 parameters compared to DLinear's 100,000, FreqHybrid has three times the model capacity. However, our results demonstrate that parameter count does not translate to predictive performance when the underlying patterns are simple, and the extra parameters increase the risk of overfitting. For the Electricity dataset, FreqHybrid achieves a training MSE of 0.205 versus a validation MSE of 0.234 (gap of 14%), while DLinear shows a training MSE of 0.206 versus a validation MSE of 0.185, performing better on validation and suggesting it has learned generalizable patterns.
        </p>

        <h3>6.4 Limits of Specialization</h3>
        <p>
          Our results suggest that specialization only helps when different parts of the signal truly require different modeling strategies.
          In FreqHybrid's design, we assumed that trends need sequential
          modeling (LSTM), seasonal patterns need attention mechanisms
          (Transformer), and residuals need simple extrapolation (Linear). This approach does not work when the data does not meaningfully separate into these categories. In our case:
        </p>
        <p>
          (1) Trends are almost linear: both datasets have trends that change slowly and predictably, so an LSTM provides little advantage over simple extrapolation. (2) Seasonal patterns are weak: daily and weekly cycles contribute only a small share of total variation, leaving the Transformer with too little structure to learn. (3) Components are interdependent: variables in the Weather dataset influence one another (e.g., temperature affects humidity), so splitting the signal into independent "trend" and "seasonal" parts removes information the model needs.
        </p>
        <p>Because these conditions hold for both datasets, the specialization built into FreqHybrid provides very little benefit and sometimes makes learning harder. Essentially, FreqHybrid solves a problem that does not exist in these datasets. The architecture would be better suited to scenarios with (1) highly nonlinear trends (e.g., chaotic systems), (2) strong multi-scaled seasonality (e.g., hourly, daily, and weekly patterns of comparable magnitude), and (3) genuine independence between trend and seasonal components. While our results are negative, they provide valuable insights into the limits of architectural complexity in time series forecasting.</p>
      </section>

      <!-- Section 7: Discussion -->
      <section id="discussion">
        <h2>
          <span class="section-number">7.</span> Discussion and Future Work
        </h2>

        <h3>7.1 Scenarios That Favor Hybrid Architectures</h3>
        <p>
          Hybrid models are not inherently ineffective but must be used in the right contexts. Frequency-aware hybrids are more likely to help when data contains structure that genuinely requires multiple modeling strategies: (1) highly nonlinear systems (e.g., financial markets, turbulent physical processes), (2) strong multi-scale patterns with multiple meaningful cycles of similar importance, (3) very long forecasting horizons (beyond 1000 hours) where linear extrapolation may break down, and (4) multimodal forecasting tasks combining heterogeneous inputs.
        </p>

        <h3>7.2 Possible Simplifications and Alternative Metrics</h3>
        <p>
          Rather than discarding the hybrid idea altogether, simpler variations may provide a better balance: two-branch models using only LSTM and linear branches, learnable decomposition filters, shared encoder architectures that branch only at the output stage, and progressive model complexity starting from linear baselines. Alternative evaluation metrics may also highlight different strengths: probabilistic forecasting (CRPS), extreme event prediction (quantile loss), and computational efficiency metrics, where DLinear's 6× speed advantage can outweigh small accuracy differences.
        </p>

        <h3>7.3 Limitations</h3>
        <p>
          Our study has several limitations: (1) we report results from single runs, though the consistency and magnitude of observed differences (15-65% relative increases in MSE) provide strong evidence for genuine performance differences, (2) we evaluate only two datasets with smooth dynamics, (3) we test only a single hybrid architecture, (4) our hyperparameter search is limited by computational constraints, (5) we do not systematically ablate individual components, and (6) we study horizons up to 720 hours. Despite these constraints, the findings offer useful guidance on when hybrid architectures are likely and unlikely to provide meaningful benefits.
        </p>

        <p>
          <strong>Conclusion:</strong> We proposed FreqHybrid, a hybrid forecasting architecture combining frequency decomposition, specialized neural components, and adaptive gating. Across all experiments, FreqHybrid consistently underperforms the simple DLinear baseline by 15% to 65%, despite requiring roughly 3 times as many parameters and 6 times longer training time. Our analysis identifies four key factors: (1) strong local linearity makes simple extrapolation highly effective, (2) optimization challenges from conflicting gradients, (3) overfitting due to excessive model capacity, and (4) limits of specialization when data does not meaningfully separate into distinct frequency components. These findings demonstrate that architectural complexity does not guarantee better forecasting performance. Linear models remain strong competitors when data is locally smooth, and specialized components only help when different frequency bands genuinely require different modeling strategies. While FreqHybrid does not outperform DLinear on the datasets studied here, our analysis identifies settings—highly nonlinear systems, data with strong multi-scale periodicity, and extremely long forecasting horizons—where hybrid architectures may still offer advantages.
        </p>
      </section>

      <!-- Works Cited -->
      <section id="references">
        <h2><span class="section-number">8.</span> Works Cited</h2>

        <ol class="references-list">
          <li>
            Abrami, G., et al. (2017). Exponential smoothing methods for time series forecasting. <em>Journal of Time Series Analysis</em>.
          </li>
          <li>
            Adhikari, R. (2013). ARIMA models for time series forecasting. <em>International Journal of Forecasting</em>.
          </li>
          <li>
            Das, A., et al. (2023). Linear models for long-horizon time series forecasting. <em>Advances in Neural Information Processing Systems</em>.
          </li>
          <li>
            Daubechies, I. (1992). <em>Ten Lectures on Wavelets</em>. Society for Industrial and Applied Mathematics.
          </li>
          <li>
            Ekambaram, V., et al. (2023). On the effectiveness of linear baselines in time series forecasting. <em>Proceedings of the International Conference on Machine Learning</em>.
          </li>
          <li>
            Elsworth, S., & Guttel, S. (2020). Long short-term memory networks for time series forecasting. <em>Neural Networks</em>.
          </li>
          <li>
            Huang, N. E., et al. (1998). The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis. <em>Proceedings of the Royal Society of London</em>.
          </li>
          <li>
            Li, S., et al. (2019). Enhancing the locality and breaking the memory bottleneck of attention on time series forecasting. <em>Advances in Neural Information Processing Systems</em>.
          </li>
          <li>
            Liu, Y., et al. (2020). Hybrid LSTM and feed-forward networks for time series prediction. <em>IEEE Transactions on Neural Networks and Learning Systems</em>.
          </li>
          <li>
            Nie, Y., et al. (2022). A time series is worth 64 words: Long-term forecasting with transformers. <em>International Conference on Learning Representations</em>.
          </li>
          <li>
            Oppenheim, A. V., & Schafer, R. W. (2009). <em>Discrete-Time Signal Processing</em> (3rd ed.). Prentice Hall.
          </li>
          <li>
            Qiu, M., et al. (2017). Convolutional neural network encoders for time series forecasting. <em>Proceedings of the IEEE International Conference on Data Mining</em>.
          </li>
          <li>
            Wu, H., et al. (2021). Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. <em>Advances in Neural Information Processing Systems</em>.
          </li>
          <li>
            Zeng, A., et al. (2023). Are transformers effective for time series forecasting? <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>.
          </li>
          <li>
            Zhou, H., et al. (2020). Informer: Beyond efficient transformer for long sequence time-series forecasting. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>.
          </li>
          <li>
            Zhou, T., et al. (2021). FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. <em>International Conference on Machine Learning</em>.
          </li>
        </ol>
      </section>

      <!-- Footer -->
      <footer>
        <hr />
        <p>
          <em
            >Thank you for reading! Questions or comments? Feel free to reach
            out.</em
          >
        </p>
        <p style="text-align: center; color: #666; font-size: 0.9em"></p>
      </footer>
    </div>
  </body>
</html>
